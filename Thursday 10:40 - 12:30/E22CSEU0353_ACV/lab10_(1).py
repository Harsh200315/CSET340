# -*- coding: utf-8 -*-
"""lab10 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E4o4YMid6ZZ4phcMTlVz74dTbNG7y4oS
"""

!pip install PyWavelets

import numpy as np
import pandas as pd
import os

import cv2
from skimage import io, restoration, metrics
import matplotlib.pyplot as plt

# Load and normalize an image
def read_and_normalize_image(file_path):
    img = io.imread(file_path)
    return img.astype(np.float32) / 255.0

# File path to the input image
img_file = '/content/Noisy-image-Gaussian-noise-with-mean-and-variance-0005.png'
input_image = read_and_normalize_image(img_file)

# Display the input image
plt.imshow(input_image, cmap='gray')
plt.title('Input Image')
plt.axis('off')
plt.show()

# Apply Gaussian blur
blurred_img = cv2.GaussianBlur(input_image, (5, 5), sigmaX=0)

plt.imshow(blurred_img, cmap='gray')
plt.title('Gaussian Blurred')
plt.axis('off')
plt.show()

# Apply median filter (convert to uint8 first)
median_img_uint8 = cv2.medianBlur((input_image * 255).astype(np.uint8), 5)

plt.imshow(median_img_uint8, cmap='gray')
plt.title('Median Filtered')
plt.axis('off')
plt.show()

# Apply wavelet-based denoising
wavelet_img = restoration.denoise_wavelet(input_image, method='BayesShrink', mode='soft')

plt.imshow(wavelet_img, cmap='gray')
plt.title('Wavelet Denoised')
plt.axis('off')
plt.show()

# Function to compute and print evaluation metrics
def compare_images(reference, processed, label):
    psnr_value = metrics.peak_signal_noise_ratio(reference, processed, data_range=1.0)
    ssim_value = metrics.structural_similarity(reference, processed, data_range=1.0, win_size=3, channel_axis=None)
    mse_value = metrics.mean_squared_error(reference, processed)
    print(f"{label}:\n PSNR: {psnr_value:.2f}\n SSIM: {ssim_value:.4f}\n MSE: {mse_value:.6f}\n")

# Normalize median filtered image before evaluation
median_img_normalized = median_img_uint8.astype(np.float32) / 255.0

# Compare all methods
compare_images(input_image, blurred_img, "Gaussian Blur")
compare_images(input_image, median_img_normalized, "Median Filter")
compare_images(input_image, wavelet_img, "Wavelet Denoising")

# importing the necessary libraries
import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Creating a VideoCapture object to read the video
cap = cv2.VideoCapture('/content/WhatsApp Video 2025-04-23 at 22.33.39_ac93a6af.mp4')

# Loop until the end of the video
while cap.isOpened():

    # Capture frame-by-frame
    ret, frame = cap.read()

    if not ret:
        print("Failed to grab frame. End of video or cannot read file.")
        break

    frame = cv2.resize(frame, (540, 380), interpolation=cv2.INTER_CUBIC)

    # Display the resulting frame
    cv2_imshow(frame)

    # Convert BGR to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Adaptive thresholding
    Thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
                                   cv2.THRESH_BINARY_INV, 11, 2)

    cv2_imshow(Thresh)

    # Define 'q' as the exit key
    if cv2.waitKey(25) & 0xFF == ord('q'):
        break

# Release the video capture object
cap.release()
cv2.destroyAllWindows()

import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Load the video
cap = cv2.VideoCapture('/content/WhatsApp Video 2025-04-23 at 22.33.39_ac93a6af.mp4')  # Use full path if not in root

# Check if video opened successfully
if not cap.isOpened():
    print("Error: Could not open video.")
else:
    print("Video opened successfully.")

frame_count = 0
max_frames = 20  # Limit to first 20 frames for testing

while cap.isOpened() and frame_count < max_frames:
    ret, frame = cap.read()
    if not ret:
        print("No more frames to read or video ended.")
        break

    # Resize the frame
    resized_frame = cv2.resize(frame, (540, 380), interpolation=cv2.INTER_CUBIC)

    # Apply Gaussian Blur
    blurred = cv2.GaussianBlur(resized_frame, (5, 5), 0)

    # Show both original and blurred frame
    print(f"Displaying frame {frame_count + 1}")
    cv2_imshow(resized_frame)
    cv2_imshow(blurred)

    frame_count += 1

cap.release()
print("Done.")

# Canny Edge Detection on Video in Google Colab

import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Load the video file
cap = cv2.VideoCapture('/content/WhatsApp Video 2025-04-23 at 22.33.39_ac93a6af.mp4')

# Frame counter to avoid displaying too many frames
frame_limit = 20
frame_count = 0

# Check if the video is opened
if not cap.isOpened():
    print("Error: Cannot open video.")
else:
    print("Video opened successfully.")

while cap.isOpened() and frame_count < frame_limit:
    ret, frame = cap.read()

    if not ret:
        print("No frame retrieved. Exiting...")
        break

    # Resize the frame
    frame = cv2.resize(frame, (540, 380), interpolation=cv2.INTER_CUBIC)

    # Display the original frame
    print(f"Original Frame {frame_count + 1}")
    cv2_imshow(frame)

    # Apply Canny Edge Detection
    edges = cv2.Canny(frame, 100, 200)

    # Display the edge-detected frame
    print(f"Edge Detection Frame {frame_count + 1}")
    cv2_imshow(edges)

    frame_count += 1

cap.release()
print("Video processing complete.")

# Bitwise NOT operation on video frames in Google Colab

import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Load the video file
video_path = '/content/WhatsApp Video 2025-04-24 at 10.54.31_977cfeec.mp4'  # Make sure this file exists in /content
capture = cv2.VideoCapture(video_path)

# Frame display limit to avoid excessive outputs
frame_limit = 20
frame_counter = 0

# Check if video was successfully loaded
if not capture.isOpened():
    print("Failed to open the video file.")
else:
    print("Processing video...")

while capture.isOpened() and frame_counter < frame_limit:
    success, frame = capture.read()

    if not success:
        print("No more frames or failed to read.")
        break

    # Resize frame
    resized = cv2.resize(frame, (540, 380), interpolation=cv2.INTER_CUBIC)

    # Convert to grayscale
    grayscale = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)

    # Threshold using Otsu's method to create a binary mask
    _, binary_mask = cv2.threshold(grayscale, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # Apply bitwise NOT using the binary mask
    inverted_output = cv2.bitwise_not(resized, resized, mask=binary_mask)

    # Display results
    print(f"\nOriginal Frame {frame_counter + 1}")
    cv2_imshow(resized)

    print(f"Bitwise NOT Frame {frame_counter + 1}")
    cv2_imshow(inverted_output)

    frame_counter += 1

# Release the video capture object
capture.release()
print("Done processing video.")

# Video Classification using CNN+LSTM on a Subset of UCF101 Dataset

import os
import shutil
import random
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, TimeDistributed, Dropout
from tensorflow.keras.utils import to_categorical

# Constants
SOURCE_DIR = '/content/UCF-101'
DEST_DIR = '/content/UCF101_subset'
SELECTED_CLASSES = ['Basketball', 'Biking', 'PlayingGuitar', 'Typing', 'JumpRope']
VIDEOS_PER_CLASS = 10
FRAME_HEIGHT = 112
FRAME_WIDTH = 112
SEQUENCE_LENGTH = 16
FRAME_INTERVAL = 5

# Step 1: Create subset of the dataset
os.makedirs(DEST_DIR, exist_ok=True)
for cls in SELECTED_CLASSES:
    class_path = os.path.join(SOURCE_DIR, cls)
    dest_class_path = os.path.join(DEST_DIR, cls)
    os.makedirs(dest_class_path, exist_ok=True)
    selected = random.sample(os.listdir(class_path), VIDEOS_PER_CLASS)
    for video in selected:
        shutil.copy(os.path.join(class_path, video), dest_class_path)

# Step 2: Extract frames and preprocess

def extract_frames(video_path):
    cap = cv2.VideoCapture(video_path)
    frames = []
    count = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        if count % FRAME_INTERVAL == 0:
            frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))
            frames.append(frame)
        count += 1
    cap.release()
    if len(frames) < SEQUENCE_LENGTH:
        return None
    return frames[:SEQUENCE_LENGTH]

X, y = [], []
for cls in SELECTED_CLASSES:
    class_dir = os.path.join(DEST_DIR, cls)
    for video_file in os.listdir(class_dir):
        video_path = os.path.join(class_dir, video_file)
        frames = extract_frames(video_path)
        if frames and len(frames) == SEQUENCE_LENGTH:
            X.append(frames)
            y.append(cls)

X = np.array(X)
y = np.array(y)

# Step 3: Label encoding and splitting
y_encoded = LabelEncoder().fit_transform(y)
y_categorical = to_categorical(y_encoded)
X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, stratify=y_encoded, random_state=42)

# Step 4: Model Definition (CNN + LSTM)
model = Sequential([
    TimeDistributed(Conv2D(32, (3,3), activation='relu'), input_shape=(SEQUENCE_LENGTH, FRAME_HEIGHT, FRAME_WIDTH, 3)),
    TimeDistributed(MaxPooling2D(2, 2)),
    TimeDistributed(Conv2D(64, (3,3), activation='relu')),
    TimeDistributed(MaxPooling2D(2, 2)),
    TimeDistributed(Flatten()),
    LSTM(64),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dense(len(SELECTED_CLASSES), activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Step 5: Train the model
model.fit(X_train, y_train, epochs=10, batch_size=4, validation_split=0.2)

# Step 6: Evaluate the model
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=1)
y_true_labels = np.argmax(y_test, axis=1)

print("Accuracy:", accuracy_score(y_true_labels, y_pred_labels))
print("Confusion Matrix:\n", confusion_matrix(y_true_labels, y_pred_labels))
print("Classification Report:\n", classification_report(y_true_labels, y_pred_labels, target_names=SELECTED_CLASSES))

import kagglehub
import os
import shutil
import random


dataset_path = kagglehub.dataset_download("pevogam/ucf101")
print("✅ Dataset downloaded at:", dataset_path)

import os
import shutil
import random


SOURCE_DIR = "/root/.cache/kagglehub/datasets/pevogam/ucf101/versions/1/UCF101/UCF-101"
DEST_DIR = "./UCF101_subset"
SELECTED_CLASSES = ['Basketball', 'Biking', 'PlayingGuitar', 'Typing', 'JumpRope']
VIDEOS_PER_CLASS = 10

os.makedirs(DEST_DIR, exist_ok=True)

for cls in SELECTED_CLASSES:
    class_path = os.path.join(SOURCE_DIR, cls)
    dest_class_path = os.path.join(DEST_DIR, cls)
    os.makedirs(dest_class_path, exist_ok=True)

    # Get list of all .avi files
    videos = [f for f in os.listdir(class_path) if f.endswith('.avi')]
    selected = random.sample(videos, VIDEOS_PER_CLASS)

    for video in selected:
        shutil.copy(os.path.join(class_path, video), dest_class_path)

print("✅ Subset created at:", DEST_DIR)

import cv2
import numpy as np

# Parameters
FRAME_SIZE = (112, 112)  # (224, 224)
SEQ_LENGTH = 16
EXTRACT_EVERY = 5

def extract_frames(video_path):
    cap = cv2.VideoCapture(video_path)
    frames = []
    frame_idx = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        if frame_idx % EXTRACT_EVERY == 0:
            frame = cv2.resize(frame, FRAME_SIZE)
            frames.append(frame)
        frame_idx += 1

    cap.release()


    while len(frames) < SEQ_LENGTH:
        frames.append(frames[-1])

    return np.array(frames[:SEQ_LENGTH])

from sklearn.preprocessing import LabelEncoder
import os
from tqdm import tqdm


SOURCE_DIR = "./UCF101_subset"
SELECTED_CLASSES = ['Basketball', 'Biking', 'PlayingGuitar', 'Typing', 'JumpRope']
VIDEOS_PER_CLASS = 10

def load_dataset(dataset_path):
    X, y = [], []
    classes = sorted(os.listdir(dataset_path))

    for cls in classes:
        class_path = os.path.join(dataset_path, cls)
        for video in tqdm(os.listdir(class_path), desc=cls):
            video_path = os.path.join(class_path, video)
            frames = extract_frames(video_path)
            X.append(frames)
            y.append(cls)

    X = np.array(X)


    le = LabelEncoder()
    y = le.fit_transform(y)

    return X, y, le

from sklearn.model_selection import train_test_split

# Load and preprocess the data
X, y, le = load_dataset(SOURCE_DIR)

# Split the data into 80/20 for training/testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

print(f"Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}")

!pip install tensorflow

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense

def build_cnn_lstm_model(input_shape, num_classes):
    model = Sequential([
        TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=input_shape),
        TimeDistributed(MaxPooling2D((2, 2))),
        TimeDistributed(Flatten()),
        LSTM(64),
        Dense(64, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, GlobalAveragePooling3D, Dense

def build_3dcnn_model(input_shape, num_classes):
    model = Sequential([
        Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape),
        MaxPooling3D((2, 2, 2)),
        Conv3D(64, (3, 3, 3), activation='relu'),
        GlobalAveragePooling3D(),
        Dense(64, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# input shape for CNN-LSTM or 3D CNN
SEQ_LENGTH = 16
FRAME_SIZE = (112, 112)
input_shape = (SEQ_LENGTH, *FRAME_SIZE, 3)

num_classes = len(le.classes_)

# CNN-LSTM or 3D CNN
model = build_cnn_lstm_model(input_shape, num_classes)
# model = build_3dcnn_model(input_shape, num_classes)  # for 3D CNN

# Train the model
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=4)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Predict on test data
y_pred = np.argmax(model.predict(X_test), axis=1)


print(classification_report(y_test, y_pred, target_names=le.classes_))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

