# -*- coding: utf-8 -*-
"""cv7_24-3-25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q7bpRHtZmLXV60WmmuYNQ3VwO6UxNrNy
"""

import cv2
import numpy as np
from skimage import exposure, color
import matplotlib.pyplot as plt

def display_image(title, image):
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.title(title)
    plt.axis('off')
    plt.show()

def process_image(image_path):
    # Load image
    image = cv2.imread(image_path)
    if image is None:
        raise FileNotFoundError(f"Image at path '{image_path}' not found.")

    # Original
    display_image('Original Image', image)

    # Apply LoG
    log = cv2.Laplacian(image, cv2.CV_64F)
    log = np.uint8(np.absolute(log))
    display_image('LoG', log)

    # Apply DoG
    gauss1 = cv2.GaussianBlur(image, (5, 5), 0)
    gauss2 = cv2.GaussianBlur(image, (9, 9), 0)
    dog = cv2.subtract(gauss1, gauss2)
    display_image('DoG', dog)

    # Brightness & Contrast Adjustment
    bright_contrast = cv2.convertScaleAbs(image, alpha=1.2, beta=30)
    display_image('Brightness & Contrast', bright_contrast)

    # Sharpening
    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
    sharpened = cv2.filter2D(image, -1, kernel)
    display_image('Sharpened', sharpened)

    # Noise Removal
    denoised = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)
    display_image('Denoised', denoised)

    # Color Enhancement
    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    l = cv2.equalizeHist(l)
    enhanced = cv2.merge((l, a, b))
    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
    display_image('Color Enhanced', enhanced)

    # Resizing and Scaling
    resized = cv2.resize(image, (256, 256))
    display_image('Resized', resized)

    # Inverse Transform
    inverse = cv2.bitwise_not(image)
    display_image('Inverse Transform', inverse)

    # Histogram Equalization
    ycrcb = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)
    y, cr, cb = cv2.split(ycrcb)
    y_eq = cv2.equalizeHist(y)
    equalized = cv2.merge((y_eq, cr, cb))
    equalized = cv2.cvtColor(equalized, cv2.COLOR_YCrCb2BGR)
    display_image('Histogram Equalized', equalized)

    # Super-Resolution (simple interpolation)
    super_res = cv2.resize(image, (image.shape[1]*2, image.shape[0]*2), interpolation=cv2.INTER_CUBIC)
    display_image('Super Resolution', super_res)

    # Color Correction
    corrected = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    display_image('Color Corrected', corrected)

# Example usage (replace with your image paths):
process_image('/content/i1.jpg')
process_image('/content/i2.jpg')
process_image('/content/i3.png')
process_image('/content/i4.png')

### Install necessary libraries (if not already installed)
!pip install opencv-python-headless matplotlib numpy

### Import Libraries
import cv2
import numpy as np
from matplotlib import pyplot as plt

### Image Enhancement Function
def enhance_image(image_path):
    # Read the image
    image = cv2.imread(image_path)
    if image is None:
        print(f"Error: Unable to load image '{image_path}'. Please check the file path.")
        return

    # Adjust Brightness and Contrast
    alpha = 1.5  # Contrast control (1.0-3.0)   high value : increase contrast
    beta = 50    # Brightness control (0-100)   high value : increase brightness
    adjusted = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)

    # Sharpening
    kernel = np.array([[0, -1, 0],
                       [-1, 5, -1],
                       [0, -1, 0]])
    sharpened = cv2.filter2D(adjusted, -1, kernel)

    # Noise Removal
    denoised = cv2.fastNlMeansDenoisingColored(sharpened, None, 10, 10, 7, 21)
    #preserves edges while smoothing the noise

    # Color Enhancement
    lab = cv2.cvtColor(denoised, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
    cl = clahe.apply(l)
    enhanced = cv2.merge((cl, a, b))
    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)

# enhances the colour of image using Contrast Limited Adaptive Histogram Equalization (CLAHE),
# converts the img to LAB colour space , CLAHE on kuminance channel , then converts back to bgr
    # Resize and Scale
    resized = cv2.resize(enhanced, (512, 512))

    # Inverse Transform
    inverted = cv2.bitwise_not(resized)

    # Histogram Equalization
    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)
    equalized = cv2.equalizeHist(gray)

    # Super-resolution (dummy placeholder as actual super-res requires deep learning)
    super_res = resized  # Replace with actual model if needed

    # Color Correction (White balance)
    result = cv2.cvtColor(super_res, cv2.COLOR_BGR2LAB)
    avg_a = np.average(result[:, :, 1])
    avg_b = np.average(result[:, :, 2])
    result[:, :, 1] = result[:, :, 1] - ((avg_a - 128) * (result[:, :, 0] / 255.0) * 1.1)
    result[:, :, 2] = result[:, :, 2] - ((avg_b - 128) * (result[:, :, 0] / 255.0) * 1.1)
    corrected = cv2.cvtColor(result, cv2.COLOR_LAB2BGR)

    # Display Results
    titles = ['Original', 'Adjusted', 'Sharpened', 'Denoised', 'Enhanced', 'Resized', 'Inverted', 'Equalized', 'Corrected']
    images = [image, adjusted, sharpened, denoised, enhanced, resized, inverted, equalized, corrected]

    for i in range(len(images)):
        plt.subplot(3, 3, i + 1)
        plt.imshow(cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB) if len(images[i].shape) == 3 else images[i], cmap='gray')
        plt.title(titles[i])
        plt.axis('off')
    plt.show()

### Example Usage - Replace 'image_path' with the actual path of the 5 images
enhance_image('/content/i5.jpg')

!pip install deeplake

#samp

import cv2
import numpy as np
import matplotlib.pyplot as plt
from skimage.feature import blob_dog, blob_log, hog
from skimage import color, exposure

def load_image(image_path):
    image = cv2.imread(image_path)
    return image

def preprocess_image(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)
    return blurred

def adaptive_color_texture_filter(image):
    gray = preprocess_image(image)
    edges = cv2.Canny(gray, 50, 150)
    return edges

def log_blob_detection(image):
    filtered = adaptive_color_texture_filter(image)
    blobs_log = blob_log(filtered, min_sigma=3, max_sigma=50, num_sigma=10, threshold=0.02)
    blobs_log[:, 2] = blobs_log[:, 2] * np.sqrt(2)
    return blobs_log

def dog_blob_detection(image):
    filtered = adaptive_color_texture_filter(image)
    blobs_dog = blob_dog(filtered, min_sigma=3, max_sigma=50, threshold=0.02)
    blobs_dog[:, 2] = blobs_dog[:, 2] * np.sqrt(2)
    return blobs_dog

def detect_purple_dots(image):
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    lower_purple = np.array([120, 50, 50])
    upper_purple = np.array([160, 255, 255])
    mask = cv2.inRange(hsv, lower_purple, upper_purple)
    blobs_log = blob_log(mask, min_sigma=1, max_sigma=10, num_sigma=10, threshold=0.02)
    blobs_log[:, 2] = blobs_log[:, 2] * np.sqrt(2)
    return blobs_log, mask

def hog_feature_extraction(image):
    gray = preprocess_image(image)
    fd, hog_image = hog(gray, orientations=12, pixels_per_cell=(8, 8),
                         cells_per_block=(2, 2), visualize=True)
    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))
    return hog_image_rescaled

def display_results(image, blobs, title, min_radius=5, max_radius=80):
    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    for blob in blobs:
        y, x, r = blob
        if min_radius < r < max_radius:
            c = plt.Circle((x, y), r, color='red', linewidth=2, fill=False)
            ax.add_patch(c)
    ax.set_title(title)
    plt.show()

image_paths = ["/content/i2.jpg", "/content/i3.png", "/content/i4.png"]

for image_path in image_paths:
    image = load_image(image_path)

    if "/content/i3.png" in image_path:  # Assuming sample1.jpg is the microscope image
        blobs_purple, mask = detect_purple_dots(image)
        plt.figure(figsize=(8,6))
        plt.imshow(mask, cmap='gray')
        plt.title(f"Purple Dot Mask - {image_path}")
        plt.show()
        display_results(image, blobs_purple, f"Purple Dot Detection - {image_path}", min_radius=1, max_radius=8)

        blobs_log = log_blob_detection(image)
        display_results(image, blobs_log, f"LoG Blob Detection - {image_path}", min_radius=5, max_radius=8)

        blobs_dog = dog_blob_detection(image)
        display_results(image, blobs_dog, f"DoG Blob Detection - {image_path}", min_radius=5, max_radius=8)
    else:
        blobs_log = log_blob_detection(image)
        display_results(image, blobs_log, f"LoG Blob Detection - {image_path}")

        blobs_dog = dog_blob_detection(image)
        display_results(image, blobs_dog, f"DoG Blob Detection - {image_path}")

        hog_image = hog_feature_extraction(image)
        plt.figure(figsize=(8,6))
        plt.imshow(hog_image, cmap='gray')
        plt.title(f"HoG Feature Extraction - {image_path}")
        plt.show()



import cv2
import numpy as np
import matplotlib.pyplot as plt
from skimage.feature import blob_dog, blob_log, hog
from skimage import color, exposure

# ... (load_image, preprocess_image, adaptive_color_texture_filter,
#      log_blob_detection, dog_blob_detection, detect_purple_dots,
#      hog_feature_extraction, display_results functions remain the same) ...

image_paths = ["/content/i2.jpg", "/content/i3.png", "/content/i4.png"]

for image_path in image_paths:
    image = load_image(image_path)

    if image_path == "/content/i2.jpg":
        # Tuning for i2.jpg
        blobs_log = log_blob_detection(image)
        display_results(image, blobs_log, f"LoG Blob Detection - {image_path}", min_radius=10, max_radius=100)  # Adjusted radii

        blobs_dog = dog_blob_detection(image)
        display_results(image, blobs_dog, f"DoG Blob Detection - {image_path}", min_radius=10, max_radius=100)  # Adjusted radii

        hog_image = hog_feature_extraction(image)
        plt.figure(figsize=(8,6))
        plt.imshow(hog_image, cmap='gray')
        plt.title(f"HoG Feature Extraction - {image_path}")
        plt.show()

    elif image_path == "/content/i3.png":
        # Tuning for i3.png
        blobs_purple, mask = detect_purple_dots(image)
        plt.figure(figsize=(8,6))
        plt.imshow(mask, cmap='gray')
        plt.title(f"Purple Dot Mask - {image_path}")
        plt.show()
        display_results(image, blobs_purple, f"Purple Dot Detection - {image_path}", min_radius=1, max_radius=8)

        blobs_log = log_blob_detection(image)
        display_results(image, blobs_log, f"LoG Blob Detection - {image_path}", min_radius=2, max_radius=15)  # Adjusted radii

        blobs_dog = dog_blob_detection(image)
        display_results(image, blobs_dog, f"DoG Blob Detection - {image_path}", min_radius=2, max_radius=15)  # Adjusted radii

    elif image_path == "/content/i4.png":
        # Tuning for i4.png
        blobs_log = log_blob_detection(image)
        display_results(image, blobs_log, f"LoG Blob Detection - {image_path}", min_radius=5, max_radius=50)  # Adjusted radii

        blobs_dog = dog_blob_detection(image)
        display_results(image, blobs_dog, f"DoG Blob Detection - {image_path}", min_radius=5, max_radius=50)  # Adjusted radii

        hog_image = hog_feature_extraction(image)
        plt.figure(figsize=(8,6))
        plt.imshow(hog_image, cmap='gray')
        plt.title(f"HoG Feature Extraction - {image_path}")
        plt.show()



import cv2
import numpy as np
import matplotlib.pyplot as plt
from skimage.feature import blob_dog, blob_log, hog
from skimage import color, exposure

# ... (load_image, preprocess_image, adaptive_color_texture_filter,
#      log_blob_detection, dog_blob_detection, detect_purple_dots,
#      hog_feature_extraction, display_results functions remain the same) ...

image_path = "/content/i4.png"  # Focus on i4.png
image = load_image(image_path)

# Fine-tuned blob detection for i4.png
blobs_log = blob_log(adaptive_color_texture_filter(image), min_sigma=3, max_sigma=30, num_sigma=10, threshold=0.05) # Fine-tuned parameters
blobs_log[:, 2] = blobs_log[:, 2] * np.sqrt(2)

blobs_dog = blob_dog(adaptive_color_texture_filter(image), min_sigma=2, max_sigma=20, threshold=0.08)  # Fine-tuned parameters
blobs_dog[:, 2] = blobs_dog[:, 2] * np.sqrt(2)

# Display results with adjusted radii
display_results(image, blobs_log, f"Fine-tuned LoG Blob Detection - {image_path}", min_radius=5, max_radius=40)
display_results(image, blobs_dog, f"Fine-tuned DoG Blob Detection - {image_path}", min_radius=3, max_radius=30)

# ... (rest of your code)



import cv2
import numpy as np

def simple_blob_detection(image_path):
    # Load the image
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    # Set up the SimpleBlobDetector parameters
    params = cv2.SimpleBlobDetector_Params()

    # Filter by Area
    params.filterByArea = True
    params.minArea = 100  # Adjust as needed

    # Filter by Circularity
    params.filterByCircularity = True
    params.minCircularity = 0.5  # Adjust as needed

    # Filter by Convexity
    params.filterByConvexity = True
    params.minConvexity = 0.8  # Adjust as needed

    # Filter by Inertia
    params.filterByInertia = True
    params.minInertiaRatio = 0.5  # Adjust as needed

    # Create a SimpleBlobDetector with the parameters
    detector = cv2.SimpleBlobDetector_create(params)

    # Detect blobs
    keypoints = detector.detect(image)

    # Draw detected blobs as red circles
    im_with_keypoints = cv2.drawKeypoints(image, keypoints, np.array([]), (0, 0, 255),
                                          cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

    # Display the image with detected blobs
    plt.imshow(im_with_keypoints, cmap='gray')
    plt.title('Simple Blob Detection')
    plt.show()


# Example usage:
simple_blob_detection('/content/i4.png') # Replace with your image path



import cv2
import numpy as np
import matplotlib.pyplot as plt

def fine_tuned_blob_detection(image_path):
    # Load the image in grayscale
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    # Set up the SimpleBlobDetector parameters
    params = cv2.SimpleBlobDetector_Params()

    # Filter by Area
    params.filterByArea = True
    params.minArea = 20  # Reduced for smaller blobs in i4.png
    params.maxArea = 500  # Added to limit large blobs

    # Filter by Circularity
    params.filterByCircularity = True
    params.minCircularity = 0.7  # Increased for more circular blobs

    # Filter by Convexity
    params.filterByConvexity = True
    params.minConvexity = 0.85  # Increased for more convex blobs

    # Filter by Inertia
    params.filterByInertia = True
    params.minInertiaRatio = 0.4  # Relaxed to allow slightly elongated blobs

    # Create a SimpleBlobDetector with the parameters
    detector = cv2.SimpleBlobDetector_create(params)

    # Detect blobs
    keypoints = detector.detect(image)

    # Draw detected blobs as red circles
    im_with_keypoints = cv2.drawKeypoints(image, keypoints, np.array([]), (0, 0, 255),
                                          cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

    # Display the image with detected blobs
    plt.imshow(im_with_keypoints, cmap='gray')
    plt.title('Fine-tuned Simple Blob Detection')
    plt.show()


# Example usage:
fine_tuned_blob_detection('/content/i4.png')

# Step 1: Install Necessary Libraries
# pip install torch torchvision deeplake matplotlib

import time
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.models as models
from torch.utils.data import DataLoader
import deeplake
import matplotlib.pyplot as plt
import numpy as np

# Step 2: Load CIFAR-100 Dataset from DeepLake
# Use deeplake.open instead of deeplake.load
from torchvision import datasets

# Step 2: Load CIFAR-100 Dataset using torchvision
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize to match input size for AlexNet/VGG16
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for RGB
])

train_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)
test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=2)
test_loader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=2)


# Step 3: Load Pretrained Models (AlexNet and VGG16)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_model(model_name, num_classes=100):
    if model_name == 'alexnet':
        model = models.alexnet(pretrained=True)
        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)
    elif model_name == 'vgg16':
        model = models.vgg16(pretrained=True)
        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)
    return model.to(device)

# Step 4: Train the Model
def train_model(model, loader, epochs=5):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0001)

    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        print(f"Epoch {epoch+1}, Loss: {running_loss / len(loader):.4f}")

# Step 5: Evaluate the Performance
def evaluate_model(model, loader):
    model.eval()
    correct, total = 0, 0
    total_time = 0
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            start_time = time.time()
            outputs = model(images)
            total_time += time.time() - start_time
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    avg_inference_time = total_time / len(loader.dataset)
    print(f"Accuracy: {accuracy:.2f}%, Avg Inference Time: {avg_inference_time:.6f}s/image")
    return accuracy, avg_inference_time

# Step 6: Compare Results
alexnet = load_model('alexnet')
print("\nTraining AlexNet...")
train_model(alexnet, train_loader)
print("Evaluating AlexNet...")
alex_acc, alex_time = evaluate_model(alexnet, test_loader)

vgg16 = load_model('vgg16')
print("\nTraining VGG16...")
train_model(vgg16, train_loader)
print("Evaluating VGG16...")
vgg_acc, vgg_time = evaluate_model(vgg16, test_loader)

print("\n--- Comparison ---")
print(f"AlexNet Accuracy: {alex_acc:.2f}%, Inference Time: {alex_time:.6f}s/image")
print(f"VGG16 Accuracy:  {vgg_acc:.2f}%, Inference Time: {vgg_time:.6f}s/image")

# Step 7: Visualization
dclasses = train_data.classes  # List of class names

def imshow(img, title):
    img = img / 2 + 0.5  # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.title(title)
    plt.axis('off')
    plt.show()

dataiter = iter(test_loader)
images, labels = next(dataiter)
imshow(torchvision.utils.make_grid(images[:8]), title=' | '.join([classes[labels[j]] for j in range(8)]))

